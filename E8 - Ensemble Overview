‘Ensemble’ es una estrategia exitosa en machine learning porque usa múltiples algoritmos para consolidar un modelo con un mismo propósito: obtener la predicción más óptima en conjunto, con mejores resultados que la precisión dada por un solo modelo.
Opera de tal manera que promedia los rendimientos de los modelos individuales para disminuir la varianza y generar una clasificación más robusta, usa entonces cientos o miles de modelos del mismo o diferente algoritmo que trabajan juntos, los cuales se basan en el concepto de ‘bootstrapping’ y son generados trabajando bajo los siguientes métodos: Bagging, Boosting y Stacking
Bootstrapping: es una técnica que se refiere a un muestreo aleatorio de un subconjuto de los datos con reemplazo donde se repiten algunas observaciones para completar el tamaño de la muestra y se obtienen n muestras que tienen características diferentes lo que permite tener una mejor comprensión y realizar una predicción muy acertada. Esta técnica es perfecta para datasets pequeños que al modelarlos pueden tener tendencia a sobreajuste.
Bagging: con la técnica de muestreo donde se obtienen diferentes subconjuntos  de menor tamaño que el dataset original; en este método cada una de las muestras es entrenada con un algoritmo similar o diferente, lo cual se obtienen rendimientos diferentes para cada modelo donde son combinados o agregados por medio de votación para el ejercicio de clasificación o ponderación para la regresión, que finalmente permiten llegar una ejercicio óptimo de predicción. El algoritmo típico aquí es Random Forest
Boosting: es un método iterativo donde se toma todo dataset de entrenamiento, luego es entrenado con un modelo y testeado con el mismo dataset, y los siguientes modelos se enfocan en aquellas erróneas clasificaciones asignándoles un mayor peso lo que el siguiente modelo realizará una mejor clasificación, de esta manera el modelo con los mejores resultados y de manera similar por votación o ponderación se toma aquel con la más acertada predicción. Los modelos que se utilizan normalmente aquí son XGBoost, GBM y ADABoost 
Finalmente, si un único modelo esta sobreajustado, empleamos Bagging, si tiene un bajo rendimiento, empleamos Boosting.
